{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 16, D_in: 784, D_out: 10\n",
      "0.0% iter: 0, loss: 65.70173344720521\n",
      "0.4% iter: 100, loss: 1.7669960614140017\n",
      "0.8% iter: 200, loss: 1.0605070370168939\n",
      "1.2% iter: 300, loss: 0.01729965486588264\n",
      "1.6% iter: 400, loss: 0.8752996788472084\n",
      "2.0% iter: 500, loss: 0.7817323066419919\n",
      "2.4% iter: 600, loss: 0.9634838647696466\n",
      "2.8% iter: 700, loss: 1.1647728310602563\n",
      "3.2% iter: 800, loss: 0.35571547859895\n",
      "3.6% iter: 900, loss: 0.1813339645032253\n",
      "4.0% iter: 1000, loss: 0.7835528939650417\n",
      "4.4% iter: 1100, loss: 0.13491676229329608\n",
      "4.8% iter: 1200, loss: 0.08052703188757149\n",
      "5.2% iter: 1300, loss: 0.23038029962953505\n",
      "5.6% iter: 1400, loss: 1.2017202361915773\n",
      "6.0% iter: 1500, loss: 0.3096639889766301\n",
      "6.4% iter: 1600, loss: 0.29091684926625305\n",
      "6.8% iter: 1700, loss: 0.17144400625759315\n",
      "7.2% iter: 1800, loss: 0.15607330031091907\n",
      "7.6% iter: 1900, loss: 0.2663921978354015\n",
      "8.0% iter: 2000, loss: 0.28952387722220096\n",
      "8.4% iter: 2100, loss: 0.5259469996750105\n",
      "8.8% iter: 2200, loss: 0.33846417336988854\n",
      "9.2% iter: 2300, loss: 0.12384982488467257\n",
      "9.6% iter: 2400, loss: 0.2451319052396174\n",
      "10.0% iter: 2500, loss: 0.501404565853929\n",
      "10.4% iter: 2600, loss: 0.15320864068075948\n",
      "10.8% iter: 2700, loss: 0.2911719079441922\n",
      "11.2% iter: 2800, loss: 0.19662481098612375\n",
      "11.6% iter: 2900, loss: 0.35947482302203304\n",
      "12.0% iter: 3000, loss: 0.22715610550805707\n",
      "12.4% iter: 3100, loss: 0.08874368118053871\n",
      "12.8% iter: 3200, loss: 0.16448494905301633\n",
      "13.2% iter: 3300, loss: 0.7605919044966045\n",
      "13.6% iter: 3400, loss: 0.2251731076770825\n",
      "14.0% iter: 3500, loss: 0.043832928270572426\n",
      "14.4% iter: 3600, loss: 0.023427538961618928\n",
      "14.8% iter: 3700, loss: 0.08846155724578607\n",
      "15.2% iter: 3800, loss: 0.38091340446367605\n",
      "15.6% iter: 3900, loss: 0.1406185220695524\n",
      "16.0% iter: 4000, loss: 0.11578220535685632\n",
      "16.4% iter: 4100, loss: 0.472723272966285\n",
      "16.8% iter: 4200, loss: 0.44038222352390166\n",
      "17.2% iter: 4300, loss: 0.05118208806997921\n",
      "17.6% iter: 4400, loss: 0.06515669192822313\n",
      "18.0% iter: 4500, loss: 0.31067320546283195\n",
      "18.4% iter: 4600, loss: 0.033374848091188083\n",
      "18.8% iter: 4700, loss: 0.07813879316269054\n",
      "19.2% iter: 4800, loss: 0.09973147903601377\n",
      "19.6% iter: 4900, loss: 0.2012951976076854\n",
      "20.0% iter: 5000, loss: 0.445852286346351\n",
      "20.4% iter: 5100, loss: 0.19358107247147008\n",
      "20.8% iter: 5200, loss: 0.09704024028995552\n",
      "21.2% iter: 5300, loss: 0.9488595093077165\n",
      "21.6% iter: 5400, loss: 0.17719957012595106\n",
      "22.0% iter: 5500, loss: 0.04608673419825785\n",
      "22.4% iter: 5600, loss: 0.09371768303235324\n",
      "22.8% iter: 5700, loss: 0.16846245682365862\n",
      "23.2% iter: 5800, loss: 0.52663825366014\n",
      "23.6% iter: 5900, loss: 0.10500459433795727\n",
      "24.0% iter: 6000, loss: 0.20522579152173254\n",
      "24.4% iter: 6100, loss: 0.17090962203815285\n",
      "24.8% iter: 6200, loss: 0.03363985839081486\n",
      "25.2% iter: 6300, loss: 0.11764805488299065\n",
      "25.6% iter: 6400, loss: 0.013939491415819167\n",
      "26.0% iter: 6500, loss: 0.20569564078672248\n",
      "26.4% iter: 6600, loss: 0.11067520273435212\n",
      "26.8% iter: 6700, loss: 0.03525435227240765\n",
      "27.2% iter: 6800, loss: 0.10657611308181886\n",
      "27.6% iter: 6900, loss: 0.06496112083763896\n",
      "28.0% iter: 7000, loss: 0.3760425684061628\n",
      "28.4% iter: 7100, loss: 0.2420787197794884\n",
      "28.8% iter: 7200, loss: 0.08998593251471855\n",
      "29.2% iter: 7300, loss: 0.08506417371605647\n",
      "29.6% iter: 7400, loss: 0.044804339516301965\n",
      "30.0% iter: 7500, loss: 0.1996973933733045\n",
      "30.4% iter: 7600, loss: 0.1512522502754892\n",
      "30.8% iter: 7700, loss: 0.10021271148777015\n",
      "31.2% iter: 7800, loss: 0.05473391947165699\n",
      "31.6% iter: 7900, loss: 0.047474854630615\n",
      "32.0% iter: 8000, loss: 0.19294079504504613\n",
      "32.4% iter: 8100, loss: 0.1586751661433534\n",
      "32.8% iter: 8200, loss: 0.027059818391991754\n",
      "33.2% iter: 8300, loss: 0.06621499796103818\n",
      "33.6% iter: 8400, loss: 0.058010242617351465\n",
      "34.0% iter: 8500, loss: 0.025504050493058875\n",
      "34.4% iter: 8600, loss: 0.18407312971525117\n",
      "34.8% iter: 8700, loss: 0.15100914055247927\n",
      "35.2% iter: 8800, loss: 0.3306173411735598\n",
      "35.6% iter: 8900, loss: 0.1256675763861019\n",
      "36.0% iter: 9000, loss: 0.032193466996868\n",
      "36.4% iter: 9100, loss: 0.5648316059154888\n",
      "36.8% iter: 9200, loss: 0.11303873740538514\n",
      "37.2% iter: 9300, loss: 0.055843071435620874\n",
      "37.6% iter: 9400, loss: 0.26611519890051083\n",
      "38.0% iter: 9500, loss: 0.0410915172671892\n",
      "38.4% iter: 9600, loss: 0.0968024482211626\n",
      "38.8% iter: 9700, loss: 0.044834951855505505\n",
      "39.2% iter: 9800, loss: 0.030773667456776885\n",
      "39.6% iter: 9900, loss: 0.21807134212704093\n",
      "40.0% iter: 10000, loss: 0.034868474631492744\n",
      "40.4% iter: 10100, loss: 0.08571916568277038\n",
      "40.8% iter: 10200, loss: 0.10001607803451434\n",
      "41.2% iter: 10300, loss: 0.14550637739307193\n",
      "41.6% iter: 10400, loss: 0.2950325211818295\n",
      "42.0% iter: 10500, loss: 0.049945334501325434\n",
      "42.4% iter: 10600, loss: 0.032992391339296503\n",
      "42.8% iter: 10700, loss: 0.06867571965704232\n",
      "43.2% iter: 10800, loss: 0.09647399612914345\n",
      "43.6% iter: 10900, loss: 0.06904741788313792\n",
      "44.0% iter: 11000, loss: 0.023892034065320818\n",
      "44.4% iter: 11100, loss: 0.04106506439664929\n",
      "44.8% iter: 11200, loss: 0.15322887133463403\n",
      "45.2% iter: 11300, loss: 0.48169289030845697\n",
      "45.6% iter: 11400, loss: 0.3219992860414333\n",
      "46.0% iter: 11500, loss: 0.024202522413827875\n",
      "46.4% iter: 11600, loss: 0.2498711923632491\n",
      "46.8% iter: 11700, loss: 0.09260316538466948\n",
      "47.2% iter: 11800, loss: 0.08962606751252941\n",
      "47.6% iter: 11900, loss: 0.12650622451552906\n",
      "48.0% iter: 12000, loss: 0.18917075076391476\n",
      "48.4% iter: 12100, loss: 0.12086518969474458\n",
      "48.8% iter: 12200, loss: 0.02378201729007715\n",
      "49.2% iter: 12300, loss: 0.010922841522950475\n",
      "49.6% iter: 12400, loss: 0.07388883158049103\n",
      "50.0% iter: 12500, loss: 0.3418919911340813\n",
      "50.4% iter: 12600, loss: 0.22347139977942926\n",
      "50.8% iter: 12700, loss: 0.2003728043065621\n",
      "51.2% iter: 12800, loss: 0.07418092997331116\n",
      "51.6% iter: 12900, loss: 0.10477645637930524\n",
      "52.0% iter: 13000, loss: 0.1766148512805537\n",
      "52.4% iter: 13100, loss: 0.24188339500196418\n",
      "52.8% iter: 13200, loss: 0.07828342846218257\n",
      "53.2% iter: 13300, loss: 0.21607595198923485\n",
      "53.6% iter: 13400, loss: 0.4953852628930879\n",
      "54.0% iter: 13500, loss: 0.019608795196585354\n",
      "54.4% iter: 13600, loss: 0.018246720879177513\n",
      "54.8% iter: 13700, loss: 0.04001309095572457\n",
      "55.2% iter: 13800, loss: 0.12272552440098032\n",
      "55.6% iter: 13900, loss: 0.09193628271066204\n",
      "56.0% iter: 14000, loss: 0.2110710243717489\n",
      "56.4% iter: 14100, loss: 0.09576783708909514\n",
      "56.8% iter: 14200, loss: 0.06413022981645408\n",
      "57.2% iter: 14300, loss: 0.31579627792236076\n",
      "57.6% iter: 14400, loss: 0.2556984762768518\n",
      "58.0% iter: 14500, loss: 0.20801333144573225\n",
      "58.4% iter: 14600, loss: 0.029093736524427368\n",
      "58.8% iter: 14700, loss: 0.1062146620406869\n",
      "59.2% iter: 14800, loss: 0.04911787504987039\n",
      "59.6% iter: 14900, loss: 0.2435056190119917\n",
      "60.0% iter: 15000, loss: 0.05116474774833735\n",
      "60.4% iter: 15100, loss: 0.1684128650257959\n",
      "60.8% iter: 15200, loss: 0.12401730685065888\n",
      "61.2% iter: 15300, loss: 0.08088812645834875\n",
      "61.6% iter: 15400, loss: 0.49172118503112944\n",
      "62.0% iter: 15500, loss: 0.04704725403173055\n",
      "62.4% iter: 15600, loss: 0.026887722142188326\n",
      "62.8% iter: 15700, loss: 0.09723789076529366\n",
      "63.2% iter: 15800, loss: 0.17386331470263797\n",
      "63.6% iter: 15900, loss: 0.1074440077401598\n",
      "64.0% iter: 16000, loss: 0.11084382794684844\n",
      "64.4% iter: 16100, loss: 0.2008043369324796\n",
      "64.8% iter: 16200, loss: 0.19821077191981734\n",
      "65.2% iter: 16300, loss: 0.11345172039573821\n",
      "65.6% iter: 16400, loss: 0.13806476007108626\n",
      "66.0% iter: 16500, loss: 0.05268108759164087\n",
      "66.4% iter: 16600, loss: 0.06510573853153448\n",
      "66.8% iter: 16700, loss: 0.39455406994061204\n",
      "67.2% iter: 16800, loss: 0.036153284510064686\n",
      "67.6% iter: 16900, loss: 0.18292540642774063\n",
      "68.0% iter: 17000, loss: 0.13245481292764408\n",
      "68.4% iter: 17100, loss: 0.09250993052968444\n",
      "68.8% iter: 17200, loss: 0.06977499655342198\n",
      "69.2% iter: 17300, loss: 0.16976788509088298\n",
      "69.6% iter: 17400, loss: 0.08260597753122792\n",
      "70.0% iter: 17500, loss: 0.0872688715065896\n",
      "70.4% iter: 17600, loss: 0.04907680124390622\n",
      "70.8% iter: 17700, loss: 0.43249782777541446\n",
      "71.2% iter: 17800, loss: 0.008436558935866709\n",
      "71.6% iter: 17900, loss: 0.10760742888082755\n",
      "72.0% iter: 18000, loss: 0.0985762772190917\n",
      "72.4% iter: 18100, loss: 0.03023028547628133\n",
      "72.8% iter: 18200, loss: 0.23917301662992052\n",
      "73.2% iter: 18300, loss: 0.12112459193610779\n",
      "73.6% iter: 18400, loss: 0.0708468359591794\n",
      "74.0% iter: 18500, loss: 0.28874208979287863\n",
      "74.4% iter: 18600, loss: 0.16791154606492184\n",
      "74.8% iter: 18700, loss: 0.024197102475023206\n",
      "75.2% iter: 18800, loss: 0.33722113631268047\n",
      "75.6% iter: 18900, loss: 0.1208757883742558\n",
      "76.0% iter: 19000, loss: 0.15916222397259536\n",
      "76.4% iter: 19100, loss: 0.15746606379135247\n",
      "76.8% iter: 19200, loss: 0.07964570815640899\n",
      "77.2% iter: 19300, loss: 0.08933021180809515\n",
      "77.6% iter: 19400, loss: 0.07732602818381193\n",
      "78.0% iter: 19500, loss: 0.11753264826266892\n",
      "78.4% iter: 19600, loss: 0.06975721023949741\n",
      "78.8% iter: 19700, loss: 0.07995480524884799\n",
      "79.2% iter: 19800, loss: 0.11939227805362174\n",
      "79.6% iter: 19900, loss: 0.3011827887355787\n",
      "80.0% iter: 20000, loss: 0.021423061771174184\n",
      "80.4% iter: 20100, loss: 0.0771343490254527\n",
      "80.8% iter: 20200, loss: 0.461963409600599\n",
      "81.2% iter: 20300, loss: 0.3478948354875416\n",
      "81.6% iter: 20400, loss: 0.13802347746371485\n",
      "82.0% iter: 20500, loss: 0.06571543443763178\n",
      "82.4% iter: 20600, loss: 0.23090861495290868\n",
      "82.8% iter: 20700, loss: 0.1370602169497643\n",
      "83.2% iter: 20800, loss: 0.24769929793724177\n",
      "83.6% iter: 20900, loss: 0.11319447633321868\n",
      "84.0% iter: 21000, loss: 0.1482701478302498\n",
      "84.4% iter: 21100, loss: 0.09590926442006231\n",
      "84.8% iter: 21200, loss: 0.045451026833637574\n",
      "85.2% iter: 21300, loss: 0.20734868803677456\n",
      "85.6% iter: 21400, loss: 0.09941322641392411\n",
      "86.0% iter: 21500, loss: 0.07229060847041546\n",
      "86.4% iter: 21600, loss: 0.05923690130903526\n",
      "86.8% iter: 21700, loss: 0.22284835778382636\n",
      "87.2% iter: 21800, loss: 0.23950338158520768\n",
      "87.6% iter: 21900, loss: 0.34391145461139255\n",
      "88.0% iter: 22000, loss: 0.06312449157791106\n",
      "88.4% iter: 22100, loss: 0.01938296116475284\n",
      "88.8% iter: 22200, loss: 0.12169541233197026\n",
      "89.2% iter: 22300, loss: 0.060009438353621165\n",
      "89.6% iter: 22400, loss: 0.3489160753386\n",
      "90.0% iter: 22500, loss: 0.2472921313933487\n",
      "90.4% iter: 22600, loss: 0.1533679512722743\n",
      "90.8% iter: 22700, loss: 0.09017588932182237\n",
      "91.2% iter: 22800, loss: 0.17007062331497172\n",
      "91.6% iter: 22900, loss: 0.039918357427079734\n",
      "92.0% iter: 23000, loss: 0.20797281563098166\n",
      "92.4% iter: 23100, loss: 0.14439617121709075\n",
      "92.8% iter: 23200, loss: 0.11764060813574674\n",
      "93.2% iter: 23300, loss: 0.08038694860960888\n",
      "93.6% iter: 23400, loss: 0.13047589400119522\n",
      "94.0% iter: 23500, loss: 0.0981710443720949\n",
      "94.4% iter: 23600, loss: 0.034719480609130965\n",
      "94.8% iter: 23700, loss: 0.10084757461736053\n",
      "95.2% iter: 23800, loss: 0.07827909696458711\n",
      "95.6% iter: 23900, loss: 0.045302716404989674\n",
      "96.0% iter: 24000, loss: 0.24170842308945623\n",
      "96.4% iter: 24100, loss: 0.13824747189836037\n",
      "96.8% iter: 24200, loss: 0.11500132733667552\n",
      "97.2% iter: 24300, loss: 0.5112487528170987\n",
      "97.6% iter: 24400, loss: 0.10755165705311183\n",
      "98.0% iter: 24500, loss: 0.13491307667530103\n",
      "98.4% iter: 24600, loss: 0.32264241216668127\n",
      "98.8% iter: 24700, loss: 0.3697464900347136\n",
      "99.2% iter: 24800, loss: 0.18933966571315236\n",
      "99.6% iter: 24900, loss: 0.10117321952460438\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "filename = [\n",
    "\t[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "\t[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "\t[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "\t[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
    "\n",
    "def MakeOneHot(Y, D_out):\n",
    "    N = Y.shape[0]\n",
    "    Z = np.zeros((N, D_out))\n",
    "    Z[np.arange(N), Y] = 1\n",
    "    return Z\n",
    "\n",
    "def draw_losses(losses):\n",
    "    t = np.arange(len(losses))\n",
    "    plt.plot(t, losses)\n",
    "    plt.show()\n",
    "\n",
    "def get_batch(X, Y, batch_size):\n",
    "    N = len(X)\n",
    "    i = random.randint(1, N-batch_size)\n",
    "    return X[i:i+batch_size], Y[i:i+batch_size]\n",
    "\n",
    "class FC():\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \"\"\"\n",
    "    def __init__(self, D_in, D_out):\n",
    "        #print(\"Build FC\")\n",
    "        self.cache = None\n",
    "        #self.W = {'val': np.random.randn(D_in, D_out), 'grad': 0}\n",
    "        self.W = {'val': np.random.normal(0.0, np.sqrt(2/D_in), (D_in,D_out)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(D_out), 'grad': 0}\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"FC: _forward\")\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"FC: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "        #self._update_params()\n",
    "        return dX\n",
    "\n",
    "    def _update_params(self, lr=0.001):\n",
    "        # Update the parameters\n",
    "        self.W['val'] -= lr*self.W['grad']\n",
    "        self.b['val'] -= lr*self.b['grad']\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build ReLU\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"ReLU: _forward\")\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        #print(\"ReLU: _backward\")\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX\n",
    "\n",
    "class Sigmoid():\n",
    "    \"\"\"\n",
    "    Sigmoid activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.cache = X\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = dout*X*(1-X)\n",
    "        return dX\n",
    "\n",
    "class tanh():\n",
    "    \"\"\"\n",
    "    tanh activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = X\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.cache = X\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def _backward(self, X):\n",
    "        X = self.cache\n",
    "        dX = dout*(1 - np.tanh(X)**2)\n",
    "        return dX\n",
    "\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Softmax activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build Softmax\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"Softmax: _forward\")\n",
    "        maxes = np.amax(X, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        Y = np.exp(X - maxes)\n",
    "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "        self.cache = (X, Y, Z)\n",
    "        return Z # distribution\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, Y, Z = self.cache\n",
    "        dZ = np.zeros(X.shape)\n",
    "        dY = np.zeros(X.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        N = X.shape[0]\n",
    "        for n in range(N):\n",
    "            i = np.argmax(Z[n])\n",
    "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
    "            M = np.zeros((N,N))\n",
    "            M[:,i] = 1\n",
    "            dY[n,:] = np.eye(N) - M\n",
    "        dX = np.dot(dout,dZ)\n",
    "        dX = np.dot(dX,dY)\n",
    "        return dX\n",
    "\n",
    "class Dropout():\n",
    "    \"\"\"\n",
    "    Dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p=1):\n",
    "        self.cache = None\n",
    "        self.p = p\n",
    "\n",
    "    def _forward(self, X):\n",
    "        M = (np.random.rand(*X.shape) < self.p) / self.p\n",
    "        self.cache = X, M\n",
    "        return X*M\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, M = self.cache\n",
    "        dX = dout*M/self.p\n",
    "        return dX\n",
    "\n",
    "class Conv():\n",
    "    \"\"\"\n",
    "    Conv layer\n",
    "    \"\"\"\n",
    "    def __init__(self, Cin, Cout, F, stride=1, padding=0, bias=True):\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        #self.W = {'val': np.random.randn(Cout, Cin, F, F), 'grad': 0}\n",
    "        self.W = {'val': np.random.normal(0.0,np.sqrt(2/Cin),(Cout,Cin,F,F)), 'grad': 0} # Xavier Initialization\n",
    "        self.b = {'val': np.random.randn(Cout), 'grad': 0}\n",
    "        self.cache = None\n",
    "        self.pad = padding\n",
    "\n",
    "    def _forward(self, X):\n",
    "#         X = np.pad(X, ((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)), 'constant')\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        Y = np.zeros((N, self.Cout, H_, W_))\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(self.Cout):\n",
    "                for h in range(H_):\n",
    "                    for w in range(W_):\n",
    "                        Y[n, c, h, w] = np.sum(X[n, :, h:h+self.F, w:w+self.F] * self.W['val'][c, :, :, :]) + self.b['val'][c]\n",
    "\n",
    "        self.cache = X\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        # dout (N,Cout,H_,W_)\n",
    "        # W (Cout, Cin, F, F)\n",
    "        X = self.cache\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        W_rot = np.rot90(np.rot90(self.W['val']))\n",
    "\n",
    "        dX = np.zeros(X.shape)\n",
    "        dW = np.zeros(self.W['val'].shape)\n",
    "        db = np.zeros(self.b['val'].shape)\n",
    "\n",
    "        # dW\n",
    "        for co in range(self.Cout):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(self.F):\n",
    "                    for w in range(self.F):\n",
    "                        dW[co, ci, h, w] = np.sum(X[:,ci,h:h+H_,w:w+W_] * dout[:,co,:,:])\n",
    "\n",
    "        # db\n",
    "        for co in range(self.Cout):\n",
    "            db[co] = np.sum(dout[:,co,:,:])\n",
    "\n",
    "        dout_pad = np.pad(dout, ((0,0),(0,0),(self.F,self.F),(self.F,self.F)), 'constant')\n",
    "        #print(\"dout_pad.shape: \" + str(dout_pad.shape))\n",
    "        # dX\n",
    "        for n in range(N):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(H):\n",
    "                    for w in range(W):\n",
    "                        #print(\"self.F.shape: %s\", self.F)\n",
    "                        #print(\"%s, W_rot[:,ci,:,:].shape: %s, dout_pad[n,:,h:h+self.F,w:w+self.F].shape: %s\" % ((n,ci,h,w),W_rot[:,ci,:,:].shape, dout_pad[n,:,h:h+self.F,w:w+self.F].shape))\n",
    "                        dX[n, ci, h, w] = np.sum(W_rot[:,ci,:,:] * dout_pad[n, :, h:h+self.F,w:w+self.F])\n",
    "\n",
    "        return dX\n",
    "\n",
    "class MaxPool():\n",
    "    def __init__(self, F, stride):\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        # X: (N, Cin, H, W): maxpool along 3rd, 4th dim\n",
    "        (N,Cin,H,W) = X.shape\n",
    "        F = self.F\n",
    "        W_ = int(float(W)/F)\n",
    "        H_ = int(float(H)/F)\n",
    "        Y = np.zeros((N,Cin,W_,H_))\n",
    "        M = np.zeros(X.shape) # mask\n",
    "        for n in range(N):\n",
    "            for cin in range(Cin):\n",
    "                for w_ in range(W_):\n",
    "                    for h_ in range(H_):\n",
    "                        Y[n,cin,w_,h_] = np.max(X[n,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)])\n",
    "                        i,j = np.unravel_index(X[n,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)].argmax(), (F,F))\n",
    "                        M[n,cin,F*w_+i,F*h_+j] = 1\n",
    "        self.cache = M\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        M = self.cache\n",
    "        (N,Cin,H,W) = M.shape\n",
    "        dout = np.array(dout)\n",
    "        #print(\"dout.shape: %s, M.shape: %s\" % (dout.shape, M.shape))\n",
    "        dX = np.zeros(M.shape)\n",
    "        for n in range(N):\n",
    "            for c in range(Cin):\n",
    "                #print(\"(n,c): (%s,%s)\" % (n,c))\n",
    "                dX[n,c,:,:] = dout[n,c,:,:].repeat(2, axis=0).repeat(2, axis=1)\n",
    "        return dX*M\n",
    "\n",
    "def NLLLoss(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N = Y_pred.shape[0]\n",
    "    M = np.sum(Y_pred*Y_true, axis=1)\n",
    "    for e in M:\n",
    "        #print(e)\n",
    "        if e == 0:\n",
    "            loss += 500\n",
    "        else:\n",
    "            loss += -np.log(e)\n",
    "    return loss/N\n",
    "\n",
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        prob = softmax._forward(Y_pred)\n",
    "        loss = NLLLoss(prob, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = prob.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class SoftmaxLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        loss = NLLLoss(Y_pred, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = Y_pred.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class Net(metaclass=ABCMeta):\n",
    "    # Neural network super class\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_params(self, params):\n",
    "        pass\n",
    "\n",
    "\n",
    "class TwoLayerNet(Net):\n",
    "\n",
    "    #Simple 2 layer NN\n",
    "\n",
    "    def __init__(self, N, D_in, H, D_out, weights=''):\n",
    "        self.FC1 = FC(D_in, H)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.FC2 = FC(H, D_out)\n",
    "\n",
    "        if weights == '':\n",
    "            pass\n",
    "        else:\n",
    "            with open(weights,'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "                self.set_params(params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.FC1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        h2 = self.FC2._forward(a1)\n",
    "        return h2\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b] = params\n",
    "\n",
    "\n",
    "class ThreeLayerNet(Net):\n",
    "\n",
    "    #Simple 3 layer NN\n",
    "\n",
    "    def __init__(self, N, D_in, H1, H2, D_out, weights=''):\n",
    "        self.FC1 = FC(D_in, H1)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.FC2 = FC(H1, H2)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.FC3 = FC(H2, D_out)\n",
    "\n",
    "        if weights == '':\n",
    "            pass\n",
    "        else:\n",
    "            with open(weights,'rb') as f:\n",
    "                params = pickle.load(f)\n",
    "                self.set_params(params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.FC1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        h2 = self.FC2._forward(a1)\n",
    "        a2 = self.ReLU2._forward(h2)\n",
    "        h3 = self.FC3._forward(a2)\n",
    "        return h3\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.ReLU2._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params\n",
    "\n",
    "\n",
    "class LeNet5(Net):\n",
    "    # LeNet5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv(1, 6, 5)\n",
    "        self.ReLU1 = ReLU()\n",
    "        self.pool1 = MaxPool(2,2)\n",
    "        self.conv2 = Conv(6, 16, 5)\n",
    "        self.ReLU2 = ReLU()\n",
    "        self.pool2 = MaxPool(2,2)\n",
    "        self.FC1 = FC(16*4*4, 120)\n",
    "        self.ReLU3 = ReLU()\n",
    "        self.FC2 = FC(120, 84)\n",
    "        self.ReLU4 = ReLU()\n",
    "        self.FC3 = FC(84, 10)\n",
    "        self.Softmax = Softmax()\n",
    "\n",
    "        self.p2_shape = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1._forward(X)\n",
    "        a1 = self.ReLU1._forward(h1)\n",
    "        p1 = self.pool1._forward(a1)\n",
    "        h2 = self.conv2._forward(p1)\n",
    "        a2 = self.ReLU2._forward(h2)\n",
    "        p2 = self.pool2._forward(a2)\n",
    "        self.p2_shape = p2.shape\n",
    "        fl = p2.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1._forward(fl)\n",
    "        a3 = self.ReLU3._forward(h3)\n",
    "        h4 = self.FC2._forward(a3)\n",
    "        a5 = self.ReLU4._forward(h4)\n",
    "        h5 = self.FC3._forward(a5)\n",
    "        # a5 = self.Softmax._forward(h5)\n",
    "        return h5\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout = self.Softmax._backward(dout)\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.ReLU4._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.ReLU3._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "        dout = dout.reshape(self.p2_shape) # reshape\n",
    "        dout = self.pool2._backward(dout)\n",
    "        dout = self.ReLU2._backward(dout)\n",
    "        dout = self.conv2._backward(dout)\n",
    "        dout = self.pool1._backward(dout)\n",
    "        dout = self.ReLU1._backward(dout)\n",
    "        dout = self.conv1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])\n",
    "\n",
    "class SGDMomentum():\n",
    "    def __init__(self, params, lr=0.001, momentum=0.99, reg=0):\n",
    "        self.l = len(params)\n",
    "        self.parameters = params\n",
    "        self.velocities = []\n",
    "        for param in self.parameters:\n",
    "            self.velocities.append(np.zeros(param['val'].shape))\n",
    "        self.lr = lr\n",
    "        self.rho = momentum\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(self.l):\n",
    "            self.velocities[i] = self.rho*self.velocities[i] + (1-self.rho)*self.parameters[i]['grad']\n",
    "            self.parameters[i]['val'] -= (self.lr*self.velocities[i] + self.reg*self.parameters[i]['val'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "(1) Prepare Data: Load, Shuffle, Normalization, Batching, Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "#mnist.init()\n",
    "X_train, Y_train, X_test, Y_test = load()\n",
    "X_train, X_test = X_train/float(255), X_test/float(255)\n",
    "X_train -= np.mean(X_train)\n",
    "X_test -= np.mean(X_test)\n",
    "\n",
    "batch_size = 16\n",
    "D_in = 784\n",
    "D_out = 10\n",
    "\n",
    "print(\"batch_size: \" + str(batch_size) + \", D_in: \" + str(D_in) + \", D_out: \" + str(D_out))\n",
    "\n",
    "### TWO LAYER NET FORWARD TEST ###\n",
    "#H=400\n",
    "#model = nn.TwoLayerNet(batch_size, D_in, H, D_out)\n",
    "H1=300\n",
    "H2=100\n",
    "# model = ThreeLayerNet(batch_size, D_in, H1, H2, D_out)\n",
    "model = LeNet5()\n",
    "\n",
    "\n",
    "losses = []\n",
    "#optim = optimizer.SGD(model.get_params(), lr=0.0001, reg=0)\n",
    "optim = SGDMomentum(model.get_params(), lr=0.0001, momentum=0.80, reg=0.00003)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# TRAIN\n",
    "ITER = 25000\n",
    "for i in range(ITER):\n",
    "    # get batch, make onehot\n",
    "    X_batch, Y_batch = get_batch(X_train, Y_train, batch_size)\n",
    "    Y_batch = MakeOneHot(Y_batch, D_out)\n",
    "\n",
    "    # forward, loss, backward, step\n",
    "    X_batch = X_batch.reshape(batch_size,1,28,28)\n",
    "    Y_pred = model.forward(X_batch)\n",
    "    loss, dout = criterion.get(Y_pred, Y_batch)\n",
    "    model.backward(dout)\n",
    "    optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"%s%% iter: %s, loss: %s\" % (100*i/ITER,i, loss))\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "# save params\n",
    "weights = model.get_params()\n",
    "with open(\"weights.pkl\",\"wb\") as f:\n",
    "\tpickle.dump(weights, f)\n",
    "\n",
    "draw_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN--> Correct: 57381 out of 60000, acc=0.95635\n"
     ]
    }
   ],
   "source": [
    "# TRAIN SET ACC\n",
    "X_train = X_train.reshape(len(X_train),1,28,28)\n",
    "Y_pred = model.forward(X_train)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_train\n",
    "result = list(result)\n",
    "print(\"TRAIN--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_train.shape[0]) + \", acc=\" + str(result.count(0)/X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET ACC\n",
    "X_test = X_test.reshape(len(X_test),1,28,28)\n",
    "Y_pred = model.forward(X_test)\n",
    "result = np.argmax(Y_pred, axis=1) - Y_test\n",
    "result = list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST--> Correct: 9588 out of 10000, acc=0.9588\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST--> Correct: \" + str(result.count(0)) + \" out of \" + str(X_test.shape[0]) + \", acc=\" + str(result.count(0)/X_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
